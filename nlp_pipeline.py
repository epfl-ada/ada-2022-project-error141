import pandas as pd
import numpy as np
import spacy
#spacy.cli.download("en_core_web_trf")

import gender_guesser.detector as gender
from allennlp.predictors.predictor import Predictor
from typing import List
from spacy.tokens import Doc, Span
from itertools import chain

#Coref resolution
#------------------------------------------------------------------------------------------------
#Get the gender of a character from the cluster generated by the AllenNLP model.
def get_gender_from_coref(cluster, text):
    #Get text linked to indexes in cluster
    ref =  np.unique([" ".join(text[i[0]:i[1]+ 1]).lower() for i in cluster])
    
    #Define dict of gendered pronouns
    gendered_pronouns = dict([
        ('M', ['he','him','his','himself']),
        ('F', ['she','her','hers','herself'])
        ])
    
    #Check if cluster contain a gendered pronouns
    gender = ''
    if any(y == x for x in ref for y in gendered_pronouns['M']): gender = 'M'
    if any(y == x for x in ref for y in gendered_pronouns['F']): gender = 'F'
    
    return gender

#Order the coreference cluster by how many words is in given reference. We assume that the shortest should be the name of the character
def order_coreference(cluster, text):
    #Get text linked to indexes in cluster
    ref =  pd.DataFrame([[" ".join(text[i[0]:i[1]+ 1]).lower(), i] for i in cluster])
    ref.columns = ['ref_text','ind']
    
    #Aggrate according to text, count number of word contained in each reference, and sort 
    ref = ref.groupby('ref_text').aggregate({'ind':'sum', 'ref_text': 'count'})
    ref.columns = ['ind', 'count']; ref.index.name = None; 
    
    ref['n_words'] = ref.index; ref.n_words= ref.n_words.apply(lambda x: len(x.split()))
    ref = ref.sort_values(by=['n_words'], ascending=True) 

    #Order indexes in cluster
    cluster = list(chain(*ref.ind.values))
    cluster = list(zip(*(iter(cluster),) * 2))
    cluster = [list(x) for x in cluster]
    
    return cluster

#Fix behaviour of coref_resoltuion from AllenNLP 
#using Modified Code from https://github.com/NeuroSYS-pl/coreference-resolution

from typing import List
from spacy.tokens import Doc, Span

def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):
    final_token = document[coref[1]]
    #if final_token.tag_ in ["PRP$", "POS"]:
        #resolved[coref[0]] = mention_span.text + "'s" + final_token.whitespace_
    #else:
    if not(final_token.tag_ in ["PRP$", "POS"]):
        resolved[coref[0]] = mention_span.text + final_token.whitespace_
    for i in range(coref[0] + 1, coref[1] + 1):
        resolved[i] = ""
    return resolved

def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:
    spans = [doc[span[0]:span[1]+1] for span in cluster]
    spans_pos = [[token.pos_ for token in span] for span in spans]
    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)
        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]
    return span_noun_indices

def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):
    head_idx = noun_indices[0]
    head_start, head_end = cluster[head_idx]
    head_span = doc[head_start:head_end+1]
    return head_span, [head_start, head_end]

def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):
    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])

def improved_replace_corefs(document, clusters, text):
    resolved = list(tok.text_with_ws for tok in document)
    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans

    character = []
    for cluster in clusters:
        noun_indices = get_span_noun_indices(document, cluster)
        
        #Check if cluster contains gendered pronouns
        cluster_gender = get_gender_from_coref(cluster, text)

        if noun_indices:
            mention_span, mention = get_cluster_head(document, cluster, noun_indices)
            
            for coref in cluster:
                if coref != mention and not is_containing_other_spans(coref, all_spans):
                    core_logic_part(document, coref, resolved, mention_span)
            
            #Put in character dataframe all mention that are gendered
            if not(cluster_gender == ""):
                character.append([" ".join(text[mention[0]:mention[1]+ 1]).lower(), cluster_gender])
                
                    
    return "".join(resolved), character

#Full coreference resolution pipeline 
def coreference_resolution(text, nlp_pipeline, predictor):
    prediction = predictor.predict(document=text) 
    clusters = list(pd.Series(prediction['clusters']).apply(order_coreference, args=(prediction['document'],)))
    
    doc = nlp_pipeline(text)
    
    coref_text, character = improved_replace_corefs(doc, clusters, prediction['document'])
    character = pd.DataFrame(character, columns=['name','gender'])
    
    return coref_text, character

#Improvment to NER functionality
#-------------------------------------------------------------------------------------------------
#Check if a string is contained in the list of words provided
def find_whole_word(string:str,list_words:list):
    return any(y == x for x in string.lower().split() for y in list_words)

#Check if character is already in the character metadata
def check_coref_in_metadata(coref_charac, metadata):
    return pd.Series({'name': coref_charac, 'ind': np.where(metadata.apply(lambda row : find_whole_word(coref_charac, row.unique_name), axis=1))[0]})

#Add to metadata character found by coreference resolution
def add_character_from_coref(coref, metadata):
    
    split_name = [x.lower().split() for x in metadata.name]
    unique_names = list(chain(*split_name))
    
    character_to_add = np.logical_not(list(coref['name'].apply(find_whole_word, args=(unique_names, ))))
    character_to_add = pd.DataFrame(coref[character_to_add])
    character_to_add.name = [x.split(', ')[0] for x in character_to_add.name]
    
    return character_to_add

#Remove the word in the list from a string
def remove_words(name, words_remove):
    name_words = name.split()

    resultwords  = [word for word in name_words if word.lower() not in words_remove]
    result = ' '.join(resultwords)
    
    return result

#Apply the remove_words on all characters
def character_remove_words(words_remove, character):
    #Remove point in name
    character.name = character['name'].apply(lambda x: x.replace('.', ''))
    #Remove all words from list
    character.name = character['name'].apply(remove_words, args=(words_remove, ))
    
    return character

#Create pattern for the add_entity module from the character list
def get_patterns(character):
    patterns = []
    for c in character.name:
        split_name = c.lower().split()
        for i in split_name:
             patterns.append({"label": "PERSON", "pattern": [{"LOWER": i.lower()}], "id": c})
        if (len(split_name) > 1):
            patterns.append(({"label": "PERSON", "pattern": [{"LOWER": split_name[0].lower()},{"LOWER": split_name[1].lower()}], "id": c}))
    return patterns

#Add entity ruler patterns based on chracter metadata to the nlp pipeline
def add_character_entity_ruler_to_nlp(nlp, character):
    ruler = nlp.add_pipe("entity_ruler", before ='ner')
    nlp.add_pipe("merge_entities")

    patterns = get_patterns(character)

    ruler.add_patterns(patterns)
    
    return nlp


#Get active/passive and descriptive adjective+nouns
#------------------------------------------------------------------------------------------------
def get_verb(token, doc, i):
    #Check if preposition associated with verb
    verb = token.lemma_
    not_empty = [x.dep_ for x in token.rights]
    if not_empty and next(token.rights).dep_ == 'prep':
        verb = verb + '+prep'
        
    #See if other verb associated to same subject
    conj_verb = [x.lemma_ for x in token.children if(x.dep_=='conj'and x.pos_ == 'VERB')]
    if conj_verb:
        verb = verb + ' ' + conj_verb[0]
        
    #Look for verb to verb pattern
    if doc[i+1].text == 'to' and doc[i+2].pos_ == 'VERB':
        verb = verb + ' ' + doc[i+2].text
        
    return verb

def get_more_people(child, verb, assos_verbs):
    indirect = [x for x in child.children]
    while indirect:
        temp = indirect; indirect = []
        for y in temp:
            if y.dep_ == 'conj' and y.ent_type_ == 'PERSON': 
                assos_verbs.append((y.text.lower(), verb))
                indirect = [i for i in y.children if i.dep_ == 'conj']

    return assos_verbs

def get_assos_verb(child, verb):
    assos_verbs = []
    #Look for direct link with person
    if child.ent_type_ == 'PERSON':
        assos_verbs.append((child.text.lower(), verb))
        
        #Look if verb is associated to more people
        get_more_people(child, verb, assos_verbs)      

    #See if there is an indirect link to a character
    else:
        indirect = [x for x in child.children]
        while indirect:
            temp = indirect; indirect = []
            for y in temp:
                if y.dep_ in ['appos','compound','conj']: 
                    if y.ent_type_ == 'PERSON':
                        assos_verbs.append((y.text.lower(), verb))

                    for z in y.children: 
                        if z.dep_ == 'conj' and z.ent_type_ == 'PERSON':
                            assos_verbs.append((z.text.lower(), verb))

                            if z.dep_ in ['appos','compound', 'conj']:
                                indirect = [z for z in y.children]

    return assos_verbs