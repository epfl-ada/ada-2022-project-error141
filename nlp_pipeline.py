import pandas as pd
import numpy as np
import spacy
#spacy.cli.download("en_core_web_trf")

import gender_guesser.detector as gender
from allennlp.predictors.predictor import Predictor
from typing import List
from spacy.tokens import Doc, Span
from itertools import chain

#Coref resolution
#------------------------------------------------------------------------------------------------
#Get the gender of a character from the cluster generated by the AllenNLP model.
def get_gender_from_coref(cluster:list, text:str):
    #Get text linked to indexes in cluster
    ref =  np.unique([" ".join(text[i[0]:i[1]+ 1]).lower() for i in cluster])
    
    #Define dict of gendered pronouns
    gendered_pronouns = dict([
        ('M', ['he','him','his','himself']),
        ('F', ['she','her','hers','herself'])
        ])
    
    #Check if cluster contain a gendered pronouns
    gender = ''
    if any(y == x for x in ref for y in gendered_pronouns['M']): gender = 'M'
    if any(y == x for x in ref for y in gendered_pronouns['F']): gender = 'F'
    
    return gender

#Order the coreference cluster by how many words is in given reference. We assume that the shortest should be the name of the character
def order_coreference(cluster:list, text:str):
    #Get text linked to indexes in cluster
    ref =  pd.DataFrame([[" ".join(text[i[0]:i[1]+ 1]).lower(), i] for i in cluster])
    ref.columns = ['ref_text','ind']
    
    #Aggrate according to text, count number of word contained in each reference, and sort 
    ref = ref.groupby('ref_text').aggregate({'ind':'sum', 'ref_text': 'count'})
    ref.columns = ['ind', 'count']; ref.index.name = None; 
    
    ref['n_words'] = ref.index; ref.n_words= ref.n_words.apply(lambda x: len(x.split()))
    ref = ref.sort_values(by=['n_words'], ascending=True) 

    #Order indexes in cluster
    cluster = list(chain(*ref.ind.values))
    cluster = list(zip(*(iter(cluster),) * 2))
    cluster = [list(x) for x in cluster]
    
    return cluster

#Fix behaviour of coref_resoltuion from AllenNLP 
#using Modified Code from https://github.com/NeuroSYS-pl/coreference-resolution

from typing import List
from spacy.tokens import Doc, Span

def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):
    final_token = document[coref[1]]
    #if final_token.tag_ in ["PRP$", "POS"]:
        #resolved[coref[0]] = mention_span.text + "'s" + final_token.whitespace_
    #else:
    if not(final_token.tag_ in ["PRP$", "POS"]):
        resolved[coref[0]] = mention_span.text + final_token.whitespace_
    for i in range(coref[0] + 1, coref[1] + 1):
        resolved[i] = ""
    return resolved

def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:
    spans = [doc[span[0]:span[1]+1] for span in cluster]
    spans_pos = [[token.pos_ for token in span] for span in spans]
    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)
        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]
    return span_noun_indices

def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):
    head_idx = noun_indices[0]
    head_start, head_end = cluster[head_idx]
    head_span = doc[head_start:head_end+1]
    return head_span, [head_start, head_end]

def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):
    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])

def improved_replace_corefs(document, clusters, text):
    resolved = list(tok.text_with_ws for tok in document)
    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans

    character = []
    for cluster in clusters:
        noun_indices = get_span_noun_indices(document, cluster)
        
        #Check if cluster contains gendered pronouns
        cluster_gender = get_gender_from_coref(cluster, text)

        if noun_indices:
            mention_span, mention = get_cluster_head(document, cluster, noun_indices)
            
            for coref in cluster:
                if coref != mention and not is_containing_other_spans(coref, all_spans):
                    core_logic_part(document, coref, resolved, mention_span)
            
            #Put in character dataframe all mention that are gendered
            if not(cluster_gender == ""):
                character.append([" ".join(text[mention[0]:mention[1]+ 1]).lower(), cluster_gender])
                
                    
    return "".join(resolved), character

#Full coreference resolution pipeline 
def coreference_resolution(text, nlp_pipeline, predictor):
    prediction = predictor.predict(document=text) 
    clusters = list(pd.Series(prediction['clusters']).apply(order_coreference, args=(prediction['document'],)))
    
    doc = nlp_pipeline(text)
    
    coref_text, character = improved_replace_corefs(doc, clusters, prediction['document'])
    character = pd.DataFrame(character, columns=['name','gender'])
    
    return coref_text, character

#Improvment to NER functionality
#-------------------------------------------------------------------------------------------------
#Check if a string is contained in the list of words provided
def find_whole_word(string:str,list_words:list):
    return any(y == x for x in string.lower().split() for y in list_words)

#Check if character is already in the character metadata
def check_coref_in_metadata(coref_charac, metadata):
    return pd.Series({'name': coref_charac, 'ind': np.where(metadata.apply(lambda row : find_whole_word(coref_charac, row.unique_name), axis=1))[0]})

#Add to metadata character found by coreference resolution
def add_character_from_coref(coref, metadata):
    
    split_name = [x.lower().split() for x in metadata.name]
    unique_names = list(chain(*split_name))
    
    character_to_add = np.logical_not(list(coref['name'].apply(find_whole_word, args=(unique_names, ))))
    character_to_add = pd.DataFrame(coref[character_to_add])
    character_to_add.name = [x.split(', ')[0] for x in character_to_add.name]
    
    return character_to_add

#Remove the word in the list from a string
def remove_words(name, words_remove):
    name_words = name.split()

    resultwords  = [word for word in name_words if word.lower() not in words_remove]
    result = ' '.join(resultwords)
    
    return result

#Apply the remove_words on all characters
def character_remove_words(words_remove, character):
    #Remove point in name
    character.name = character['name'].apply(lambda x: x.replace('.', ''))
    #Remove all words from list
    character.name = character['name'].apply(remove_words, args=(words_remove, ))
    
    return character

#Create pattern for the add_entity module from the character list
def get_patterns(character):
    patterns = []
    for c in character.name:
        split_name = c.lower().split()
        for i in split_name:
             patterns.append({"label": "PERSON", "pattern": [{"LOWER": i.lower()}], "id": c})
        if (len(split_name) > 1):
            patterns.append(({"label": "PERSON", "pattern": [{"LOWER": split_name[0].lower()},{"LOWER": split_name[1].lower()}], "id": c}))
    return patterns

#Add entity ruler patterns based on chracter metadata to the nlp pipeline
def add_character_entity_ruler_to_nlp(nlp, character):
    ruler = nlp.add_pipe("entity_ruler", before ='ner')
    nlp.add_pipe("merge_entities")

    patterns = get_patterns(character)

    ruler.add_patterns(patterns)
    
    return nlp


#Get active/passive and descriptive adjective+nouns
#------------------------------------------------------------------------------------------------
def get_verb(token, doc, i):
    #Check if preposition associated with verb
    verb = token.lemma_
    not_empty = [x.dep_ for x in token.rights]
    if not_empty and next(token.rights).dep_ == 'prep':
        verb = verb + '+prep'
        
    #See if other verb associated to same subject
    conj_verb = [x.lemma_ for x in token.children if(x.dep_=='conj'and x.pos_ == 'VERB')]
    if conj_verb:
        verb = verb + ' ' + conj_verb[0]
        
    #Look for verb to verb pattern
    if i + 2 < len(doc):
        if doc[i+1].text == 'to' and doc[i+2].pos_ == 'VERB':
            verb = verb + ' ' + doc[i+2].text
        
    return verb

def get_more_people(child, verb, assos_verbs):
    indirect = [x for x in child.children]
    while indirect:
        temp = indirect; indirect = []
        for y in temp:
            if y.dep_ == 'conj' and y.ent_type_ == 'PERSON': 
                assos_verbs.append((y.text.lower(), verb))
                indirect = [i for i in y.children if i.dep_ == 'conj']

    return assos_verbs

def get_assos_verb(child, verb):
    assos_verbs = []
    #Look for direct link with person
    if child.ent_type_ == 'PERSON':
        assos_verbs.append((child.text.lower(), verb))
        
        #Look if verb is associated to more people
        get_more_people(child, verb, assos_verbs)      

    #See if there is an indirect link to a character
    else:
        indirect = [x for x in child.children]
        while indirect:
            temp = indirect; indirect = []
            for y in temp:
                if y.dep_ in ['appos','compound','conj']: 
                    if y.ent_type_ == 'PERSON':
                        assos_verbs.append((y.text.lower(), verb))

                    for z in y.children: 
                        if z.dep_ == 'conj' and z.ent_type_ == 'PERSON':
                            assos_verbs.append((z.text.lower(), verb))

                            if z.dep_ in ['appos','compound', 'conj']:
                                indirect = [z for z in y.children]

    return assos_verbs

#Get active/passive verbs + descriptive noun+adjective associated to characters
def extraction_words(doc, family_occupation):
    active = []; passive = []; description = []
    for i, token in enumerate(doc):

        #Get active/passive verbs
        if token.pos_ == 'VERB':

            #Get verb and associated verbs
            verb = get_verb(token, doc, i)

            for child in token.children:

                #Get passive for peter -> "Martine throwed a rock at Peter.
                if child.dep_ == 'prep':
                    for x in child.children:
                        if x.dep_ == 'pobj' and x.ent_type_ == 'PERSON':
                            passive.append([(x.text.lower(), verb)])

                #Gramatically active verbs
                if child.dep_ == 'nsubj':
                    active.append(get_assos_verb(child, verb))

                if child.dep_ == 'agent':
                    for x in child.children:
                        if x.dep_ == 'pobj':
                            active.append(get_assos_verb(x, verb)) 


                #Gramatically passive verb
                if child.dep_ == 'nsubjpass':
                    passive.append(get_assos_verb(child, verb))

                if child.dep_ == 'dobj':
                    passive.append(get_assos_verb(child, verb))

        #Get descriptive adjectives/nouns + 
        #get throw for martine ('Martine who once throw a rock at Peter')
        elif token.ent_type_ == 'PERSON':
            for child in token.children:

                if child.pos_ == 'ADJ':
                    description.append([(token.text.lower(), child.lemma_ )])

                elif child.pos_ == 'NOUN' and child.dep_ == 'appos':

                    noun = child.lemma_
                    modifiers = [x.text for x in child.children if x.dep_ == 'amod']
                    if modifiers:
                        noun = ' '.join(modifiers) + ' ' + noun 
                    if child.lemma_ in family_occupation:
                        description.append([(token.text.lower(), noun)])

                #Martine who did not like the cake
                    for x in child.children:
                        if x.pos_ == 'VERB' and x.dep_ == 'relcl':
                            for y in x.children:
                                if y.dep_ == 'nsubj':
                                    active.append([(token.text.lower(), x.lemma_.lower())])
                                elif y.dep_== 'nsubjpass':
                                    passive.append([(token.text.lower(), x.lemma_.lower())])

                elif child.pos_ == 'VERB' and child.dep_ == 'relcl':
                    for y in child.children:
                                if y.dep_ == 'nsubj':
                                    active.append([(token.text.lower(), child.lemma_.lower())])
                                elif y.dep_== 'nsubjpass':
                                    passive.append([(token.text.lower(), child.lemma_.lower())])


        elif token.pos_ == 'NOUN':
            noun = token.lemma_ 
            modifiers = [x.text for x in token.children if x.dep_ == 'amod']
            if modifiers:
                noun = ' '.join(modifiers) + ' ' + noun
            for child in token.children:
                if child.ent_type_ == 'PERSON' and child.dep_ == 'appos':
                    if token.lemma_.lower() in family_occupation:
                        description.append([(child.text.lower(), noun)])

        elif token.pos_ == 'AUX':
            word = []; subject = []
            for child in token.children:
                if child.dep_ == 'nsubj':
                    subject = [i[0].lower() for i in get_assos_verb(child, '')]
                elif child.dep_ in ['attr','acomp']:
                    if child.pos_ == 'ADJ' or child.lemma_.lower() in family_occupation:
                        word = child.text

            if subject and word:
                description.append([(subj, word) for subj in subject])

    active = list(chain(*active)); passive = list(chain(*passive))
    description = list(chain(*description))
    
    #Concanate all verb associated to a given character
    active = pd.DataFrame(active, columns=['charac_name', 'verb']).groupby('charac_name')['verb'].apply(list)
    active = active.apply(lambda x: list(chain(*[i.split() for i in x])))
    passive = pd.DataFrame(passive, columns=['charac_name', 'verb']).groupby('charac_name')['verb'].apply(list)
    passive = passive.apply(lambda x: list(chain(*[i.split() for i in x])))

    description = pd.DataFrame(description, columns=['charac_name', 'verb']).groupby('charac_name')['verb'].apply(list)
    description = description.apply(lambda x: list(chain(*[i.split() for i in x])))
    
    return active, passive, description

def get_unique_name (name, list_names):
    name = name.lower()
    mask_unique_name = np.where([list_names.count(x) < 2 for x in name.split()])[0]
    
    return " ".join(np.array(name.split())[mask_unique_name])

def replace_name(name, names_df):

    temp  = [y for x in name.split() for y in names_df if find_whole_word(x, y.lower().split())]

    return temp[0] if temp else None

def character_replace_name(df, all_names): 
    name = [replace_name(x, all_names) for x in df.index]
    return name

#Link extraction of words from nlp to character metadata
def link_to_df(df, active, passive, description):
    #Only keep the unique name
    names = list(chain(*[x.lower().split() for x in list(df.index)]))

    df.reset_index(inplace=True); df = df.rename(columns = {'index':'name'})
    df.name = df.name.apply(get_unique_name, args=(names,))
    
    #Format active, passive, decription dataframe
    active.index = character_replace_name(active, list(df.name))
    active = pd.DataFrame(active); active.reset_index(inplace=True); active = active.rename(columns = {'index':'name', 'verb': 'active'})
    active = active.groupby(['name'])['active'].apply(lambda x: ','.join(x.astype(str))).reset_index()

    passive.index = character_replace_name(passive, list(df.name))
    passive = pd.DataFrame(passive); passive.reset_index(inplace=True); passive = passive.rename(columns = {'index':'name', 'verb': 'passive'})
    passive = passive.groupby(['name'])['passive'].apply(lambda x: ','.join(x.astype(str))).reset_index()

    description.index = character_replace_name(description, list(df.name))
    description = pd.DataFrame(description); description.reset_index(inplace=True); description = description.rename(columns = {'index':'name', 'verb':'description'})
    description = description.groupby(['name'])['description'].apply(lambda x: ','.join(x.astype(str))).reset_index()
    
    #Merge on name
    df = pd.merge(df, active, how='left', on = 'name')
    df = pd.merge(df, passive, how='left', on = 'name')
    df = pd.merge(df, description, how='left', on = 'name')
    
    return df

#Concatenate all steps of the nlp pipeline
def process_summary(summary, character, predictor, words_remove, family_occupation, gender_guesser):
    #Remove all charcater with non-defined name in characater metadata
    character = character[character['name'].notna()]
    
    #Load the pre-trained spacy module
    nlp = spacy.load("en_core_web_trf")
    
    #Run coref resolution
    coref_summary, coref_character = coreference_resolution(summary, nlp, predictor)
    
    #Add character from coref resolution to character metadata
    to_add = add_character_from_coref(coref_character,character)
    character = pd.concat([character, to_add], sort=False)

    #Remove all words from the passed list
    character = character_remove_words(words_remove, character)

    #Add entity ruler
    nlp = add_character_entity_ruler_to_nlp(nlp, character)
    character = character.set_index('name')
    
    #Run summary through nlp pipeline
    doc = nlp(coref_summary)
    
    #Get count of mentions of character in summary
    named_ent = [X.text if X.id_ == '' else X.id_ for X in doc.ents if X.label_== 'PERSON']
    c, counts = np.unique(named_ent, return_counts = True)
    df = pd.DataFrame(counts, index = c, columns=['mention'])
    df = df.sort_values(by=['mention'], ascending=False)
    
    #Merge with character metadata
    df = pd.merge(df, character, how='left', left_index=True, right_index=True)
    
    #Get passive/active verbs and description
    active, passive, description = extraction_words(doc, family_occupation)
    
    #Link verb/noun/adjective list to character
    df = link_to_df(df, active, passive, description)
    
    #Use gender_guesser to guess remaining character with non defined gender 
    df.loc[df['gender'].isna(),'gender'] = df[df['gender'].isna()].name.apply(lambda x: gender_guesser.get_gender(x))
    df.loc[~df['gender'].isin(['M','F']),'gender'] = df.gender.apply(
        lambda x: 'M' if (x == 'male' or x =='mostly_male') else ('F' if (x == 'female' or x =='mostly_female')  else np.NaN))
    
    #Remove character with no defined gender
    df = df[df['gender'].notna()]
    
    #Remove all charcater with non-defined name in characater metadata
    df = df[df['name']!= '']
    
    return df
